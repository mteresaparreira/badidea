{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de5e5b2",
   "metadata": {},
   "source": [
    "## This notebook performs downsampling on the OpenFace Extracted features' dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3396492",
   "metadata": {},
   "source": [
    "1. Downsampling - to a specified ```sampling_size```\n",
    "2. Preprocesses the downsampled data\n",
    "    - Excludes datapoints in ```failure videos``` which are before the ```failureOccurrence_timestamp```\n",
    "3. Merges all ```participants``` all ```videos``` that are downsampled to a particular ```sampling_size``` and preprocessed (as per #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eff8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e06fcb",
   "metadata": {},
   "source": [
    "### Specify paths\n",
    "\n",
    "- ```downsampling_directory``` - path to the directory where the downsampled ```df's``` are stored for each ```participant``` \n",
    "    - the directories are then created based on the convention of ```/{frequency}fps_downsampling/```  \n",
    "\n",
    "```participant_features_directory``` - path where the original openFace extracted features of the participants are stored locally  \n",
    "```participant_database``` - stores the list of all the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea75a85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now, open openface_features files for each participant\n",
    "files_to_ignore = ['.DS_Store']\n",
    "participant_features_directory = '../../data/openFace_features_data/csv_features_clean/'\n",
    "participant_database = os.listdir(participant_features_directory)\n",
    "participant_database = [participant_folder for participant_folder in participant_database if participant_folder not in files_to_ignore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed111532",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "\n",
    "- ```required_features``` - list of all the features that are to be considered and downsampled\n",
    "- ```final_features``` - list of ```metadata columns + required_features```\n",
    "- ```participant_folder``` - contains the list of all the extracted features dataframe of the ```response_video```s of the ```participant```\n",
    "- ```csv_file``` - openface extracted feature files of the ```response_video``` of a ```participant```\n",
    "- ```target_class``` - OHE value of the class the ```response_video``` belongs to.\n",
    "- ```required_df``` - a dataframe that contains all the ```final_features``` from the original ```df``` dataframe\n",
    "\n",
    "\n",
    "- ```sampling_size``` - indicates how many ```datapoints in a second``` is to be considered and retained\n",
    "- ```frequency = 1 / sampling_size```\n",
    "- ```last_timestamp``` - last recorded time of final extracted frame of the participant's ```response_video``` in seconds\n",
    "- ```valid_timestamps``` - list of all the ```sampling_size``` intervals\n",
    "- ```downsampled_df``` - final ```df``` that contains downsampled data with ```final_features``` values\n",
    "- ```(from_time, time]``` - time range of the datapoints based on the original_df that is to be considered for the window\n",
    "- ```dataPoints_within_window``` - dataPoints within the timerange to be considered for downsampling to a single datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f7d1dc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Participant ID = 9214: 100%|██████████| 29/29 [05:15<00:00, 10.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter out the specific warning category\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "required_features = ['timestamp', 'gaze_0_x','gaze_0_y','gaze_0_z','gaze_1_x','gaze_1_y','gaze_1_z','gaze_angle_x','gaze_angle_y','pose_Tx', 'pose_Ty', 'pose_Tz','pose_Rx', 'pose_Ry', 'pose_Rz','AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r','AU01_c','AU02_c','AU04_c','AU05_c','AU06_c','AU07_c','AU09_c','AU10_c','AU12_c','AU14_c','AU15_c','AU17_c','AU20_c','AU23_c','AU25_c','AU26_c','AU28_c','AU45_c']\n",
    "# Some initial information regarding the participant and their corresponding responseVideo information to be included\n",
    "final_features = ['participant_id', 'response_video', 'class']\n",
    "\n",
    "class_types = {\n",
    "    'ch': 0,\n",
    "    'cr': 0,\n",
    "    'fh': 1,\n",
    "    'fr': 2\n",
    "}\n",
    "sampling_size = float(input('Enter the sampling size: '))\n",
    "# round it to the nearest Xth second (frequency = 1/sampling_size)\n",
    "frequency = int(1 / sampling_size)\n",
    "\n",
    "with tqdm(total=len(participant_database)) as pbar:\n",
    "    for participant in sorted(participant_database):\n",
    "\n",
    "        participant_folder_path = f'{participant_features_directory}{participant}/'\n",
    "        participant_folder = os.listdir(participant_folder_path)\n",
    "        for csv_file in sorted(participant_folder):\n",
    "\n",
    "            if csv_file in files_to_ignore:\n",
    "                continue\n",
    "\n",
    "            # Identify the class to which the csv_file belongs to\n",
    "            responseVideo_name = csv_file.split('_')[0]\n",
    "            if responseVideo_name[:2] in class_types:\n",
    "                target_class = class_types[responseVideo_name[:2]]\n",
    "\n",
    "            # Read the features csv file\n",
    "            csv_file_path = f'{participant_folder_path}{csv_file}'\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "\n",
    "            # Create a new dataframe - 'required_df' that retains all the feature columns that are required from the original dataframe - 'df'\n",
    "            required_df = df[required_features].copy()\n",
    "\n",
    "            # add participant column\n",
    "            required_df['participant_id'] = participant\n",
    "            required_df['class'] = target_class\n",
    "            required_df['response_video'] = responseVideo_name\n",
    "\n",
    "            # Reorganize the column order\n",
    "            required_df = required_df[final_features + required_features]\n",
    "\n",
    "            ## TODO: Downsampling\n",
    "\n",
    "            # Obtain the last_timestamp for every response_video data\n",
    "            last_timestamp = required_df['timestamp'].values[-1] # NOTE: this is in seconds\n",
    "            last_timestamp = round(last_timestamp * frequency) / frequency\n",
    "            # Now we get a list of timestamps once every 'sampling_size' seconds, until the last timestamp\n",
    "            valid_timestamps = np.arange(0, last_timestamp + sampling_size, sampling_size)\n",
    "\n",
    "            # A new dataframe to store the downsampled datapoints from the 'required_df'\n",
    "            downsampled_df = pd.DataFrame()\n",
    "\n",
    "            for time in valid_timestamps:\n",
    "                if time == valid_timestamps[0]:\n",
    "                    # Retrieve the rows between the range (from_time, time]\n",
    "                    downsampled_df = pd.concat([downsampled_df, required_df[required_df['timestamp'] <= time]])\n",
    "                    from_time = time\n",
    "                else:\n",
    "                    dataPoints_within_window = required_df[(required_df['timestamp'] > from_time) & (required_df['timestamp'] <= time)]\n",
    "\n",
    "                    # Check if there exists any datapoints within the range specified: (from_time, time]\n",
    "                    if len(dataPoints_within_window) > 0:\n",
    "\n",
    "                        # Now that you have datapoints within the specified window range: (from_time, time]\n",
    "                        # Calculate some statistics on the feature columns present in the dataframe\n",
    "                        # i.e: mean(), max(), etc.. on the respective feature columns\n",
    "\n",
    "                        # NOTE: here we reduce the range of dataPoints to a single dataPoint after calculating the statistics\n",
    "                        for column in dataPoints_within_window.columns:\n",
    "\n",
    "                            # for the feature columns that contain values based on classification\n",
    "                            # i.e: AU##_c :- columns, find the max value amongst them (i.e: 0 or 1)\n",
    "                            if '_c' in column:\n",
    "                                dataPoints_within_window[column] = dataPoints_within_window[column].max()\n",
    "                            # for all other columns except few, calculate the aggregate value\n",
    "                            elif column not in ['participant_id', 'class', 'response_video', 'timestamp']:\n",
    "                                dataPoints_within_window[column] = dataPoints_within_window[column].mean()\n",
    "                            elif column == 'timestamp':\n",
    "                                dataPoints_within_window[column] = time\n",
    "\n",
    "                        # Now we add the dataPoints_within_window :- that have been reduced to a single datapoint\n",
    "                        # to the final - downsampled_df\n",
    "                        downsampled_df = pd.concat([downsampled_df.T, dataPoints_within_window.iloc[0, :]], axis = 1).T\n",
    "                    else:\n",
    "                        print(f'No datapoints within the specified range! ({from_time}, {time}]')\n",
    "\n",
    "                    # Update the range of the time :- to shift the window\n",
    "                    from_time = time\n",
    "                # print(downsampled_df.shape)\n",
    "            \n",
    "            #make new folder in data/openFace_features_data directory, called {frequency}fps_downsampling\n",
    "            downsampling_directory = f'../../data/openFace_features_data/downsampled_feature_data/{frequency}fps_downsampling/'\n",
    "            if not os.path.exists(downsampling_directory):\n",
    "                os.mkdir(downsampling_directory)\n",
    "            \n",
    "            # If the participant directory does not exist for the downsampling, create a directory\n",
    "            if not os.path.exists(f'./{downsampling_directory}/{participant}/'):\n",
    "                os.mkdir(f'./{downsampling_directory}/{participant}/')\n",
    "\n",
    "            # Save the downsampled features file as - ch1_5fps.csv in the directory: participant\n",
    "            downsampled_df.to_csv(f'./{downsampling_directory}/{participant}/{responseVideo_name}_{frequency}fps.csv', index = False)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Participant ID = {participant}')\n",
    "\n",
    "# After you're done with the code, you can reset the warning filters\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1f17d",
   "metadata": {},
   "source": [
    "#### Preprocessing failure data\n",
    "\n",
    "Here, we preprocess all the failure dataframes by removing datapoints before the task failure occurrence\n",
    "\n",
    "- For both ```human failure (fh)``` and ```robot failure (fr)``` - ```response_video``` dataframes, based on the ```failureOccurrence``` timestamp, we remove all the datapoints in the openface extracted features - ```df``` where ```row[timestamp] < failureOccurrence_time```\n",
    "\n",
    "The ```failure_timestamps``` information can be found in the ```New_Stimulus_Dataset_Information.csv```\n",
    "The ```failure_timestamps``` is of type ```dict``` where the ```dict.key: failure stimulus video name``` and ```dict.value: timestamp of the failure occurrence (in seconds)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "920cf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure_timestamps: Timestamp of failure occurences - annotated manually\n",
    "failure_timestamps = {'fh1': 3.3, 'fh4': 3.4, 'fh5': 2.8, 'fh6': 3.3, 'fh7': 5.0, 'fh8': 6.0, 'fh9': 0.8, 'fh10': 2.9, 'fh2': 6.3, 'fh3': 5.3, 'fr1': 7.8, 'fr2': 4.8, 'fr7': 6.0, 'fr10': 4.7, 'fr3': 5.5, 'fr4': 8.0, 'fr5': 8.0, 'fr6': 17.0, 'fr8': 2.3, 'fr9': 6.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01f235",
   "metadata": {},
   "source": [
    "- ```downsampled_participant_directory``` - directory where the downsampled data is stored\n",
    "- ```downsampled_participant_database``` - A list containing all participants whose data has been downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d293643",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_participant_directory = downsampling_directory\n",
    "# preprocessed_downsampled_participant_directory = \n",
    "downsampled_participant_database = os.listdir(downsampled_participant_directory)\n",
    "downsampled_participant_database = [participant_folder for participant_folder in downsampled_participant_database if participant_folder not in files_to_ignore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5f1a2",
   "metadata": {},
   "source": [
    "#### Create a new downsampling_preprocessed directory\n",
    "- This directory stores all ```participants``` extracted features in which the datapoints from the ```failure response - dfs``` are removed  \n",
    "where - ```row['timestamp'] < failure_type_failure_occurrence_timestamp```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3f637",
   "metadata": {},
   "source": [
    "- ```preprocessed_downsampled_participant_directory``` - directory where the downsampled & preprocessed data will be stored based on ```participant/responseVideo_name_frequencyfps_preprocessed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "331fdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_downsampled_participant_directory = f'../../data/openFace_features_data/downsampled_feature_data/{frequency}fps_downsampling_preprocessed/'\n",
    "if not os.path.exists(preprocessed_downsampled_participant_directory):\n",
    "    os.mkdir(preprocessed_downsampled_participant_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3190b92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:459: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "Participant ID = 9214: 100%|██████████| 29/29 [00:04<00:00,  6.30it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(downsampled_participant_database)) as pbar:\n",
    "    for participant in sorted(downsampled_participant_database):\n",
    "        participant_folder_path = f'{downsampled_participant_directory}{participant}/'\n",
    "        participant_folder = os.listdir(participant_folder_path)\n",
    "\n",
    "        if not os.path.exists(f'{preprocessed_downsampled_participant_directory}{participant}/'):\n",
    "            os.mkdir(f'{preprocessed_downsampled_participant_directory}{participant}/')\n",
    "\n",
    "        for csv_file in sorted(participant_folder):\n",
    "            responseVideo_name = csv_file.split('_')[0]\n",
    "            if csv_file in files_to_ignore:\n",
    "                continue\n",
    "            elif class_types[responseVideo_name[:2]] == 0:\n",
    "                downsampled_df = pd.read_csv(f'{participant_folder_path}{csv_file}')\n",
    "                downsampled_df.to_csv(f'{preprocessed_downsampled_participant_directory}{participant}/{responseVideo_name}_{frequency}fps_preprocessed.csv', index = False)\n",
    "            else:\n",
    "                downsampled_df = pd.read_csv(f'{participant_folder_path}{csv_file}')\n",
    "                preprocessed_downsampled_df = downsampled_df.copy()\n",
    "                preprocessed_downsampled_df = preprocessed_downsampled_df[preprocessed_downsampled_df['timestamp'] > failure_timestamps[responseVideo_name]]\n",
    "                preprocessed_downsampled_df.to_csv(f'{preprocessed_downsampled_participant_directory}{participant}/{responseVideo_name}_{frequency}fps_preprocessed.csv', index = False)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Participant ID = {participant}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d111fb0",
   "metadata": {},
   "source": [
    "#### Merge\n",
    "\n",
    "- Here we merge - all ```participants``` all ```responseVideos``` into a single dataframe called as ```allParticipants_frequencyfps_downsampled_preprocessed``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "046370f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_downsampled_participant_database = os.listdir(preprocessed_downsampled_participant_directory)\n",
    "preprocessed_downsampled_participant_database = [participant_folder for participant_folder in preprocessed_downsampled_participant_database if participant_folder not in files_to_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cee6928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Participant ID = 9214: 100%|██████████| 29/29 [00:03<00:00,  7.30it/s]\n"
     ]
    }
   ],
   "source": [
    "allParticipants_df = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(preprocessed_downsampled_participant_database)) as pbar:\n",
    "    for participant in sorted(preprocessed_downsampled_participant_database):\n",
    "        participant_folder_path = f'{preprocessed_downsampled_participant_directory}{participant}/'\n",
    "        participant_folder = os.listdir(participant_folder_path)\n",
    "\n",
    "        for csv_file in sorted(participant_folder):\n",
    "            if csv_file in files_to_ignore:\n",
    "                continue\n",
    "            current_df = pd.read_csv(f'{participant_folder_path}{csv_file}')\n",
    "            allParticipants_df = pd.concat([allParticipants_df, current_df])\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Participant ID = {participant}')\n",
    "allParticipants_df.to_csv(f'../../data/allParticipants_{frequency}fps_downsampled_preprocessed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e26531",
   "metadata": {},
   "source": [
    "#### Here, we normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68e3ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "allParticipants_df_norm = copy.deepcopy(allParticipants_df)\n",
    "\n",
    "columns_to_normalize = allParticipants_df_norm.columns[4:]\n",
    "scaler = StandardScaler()\n",
    "allParticipants_df_norm[columns_to_normalize] = scaler.fit_transform(allParticipants_df_norm[columns_to_normalize])\n",
    "\n",
    "allParticipants_df_norm.to_csv(f'../../data/allParticipants_{frequency}fps_downsampled_preprocessed_norm.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
