{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77a9d8f",
   "metadata": {},
   "source": [
    "### Conversion of GridSearch log data to a df\n",
    "\n",
    "The below code converts the logs generated whilst performing GridSearch for complex model architecture & converts the important information from the logs to a value in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5100751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85d99a",
   "metadata": {},
   "source": [
    "Merge all the ```results_LSTM.txt``` files from all the scripts to a single .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf9de99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files combined successfully!\n",
      "Text files combined successfully!\n",
      "Text files combined successfully!\n"
     ]
    }
   ],
   "source": [
    "files_to_ignore = ['.DS_Store']\n",
    "\n",
    "# Specify the LSTM script whose logs are to be considered\n",
    "logs_path = '../../model_results/lstm_results/script_2/'\n",
    "\n",
    "# Obtain all the consequent log folders\n",
    "log_folders = os.listdir(logs_path)\n",
    "log_folders = [folder for folder in log_folders if folder not in files_to_ignore]\n",
    "# print(log_folders)\n",
    "\n",
    "# Specify the final merged results.txt file path\n",
    "final_results_file_path = logs_path + 'final_merged_results'\n",
    "\n",
    "if not os.path.exists(final_results_file_path):\n",
    "    os.makedirs(final_results_file_path)\n",
    "\n",
    "final_results_txt_file_path = final_results_file_path + '/final_results.txt'\n",
    "\n",
    "# Merge all the results to a single results.txt file\n",
    "def merge_results(source_path, destination_path):\n",
    "    try:\n",
    "        with open(destination_path, 'a', encoding='utf-8') as destination_file:\n",
    "            with open(source_path, 'r', encoding='utf-8') as source_file:\n",
    "                    content = source_file.read()\n",
    "                    destination_file.write(content)\n",
    "            destination_file.write('\\n')  # Optionally add a newline between file contents\n",
    "    except Exception as e:\n",
    "        print(f'Exception {e} thrown for {source_path}')\n",
    "    print(\"Text files combined successfully!\")\n",
    "\n",
    "# Iterate over all the log folders in the model script folder and obtain the path of the results_LSTM.txt paths\n",
    "for folder in sorted(log_folders):\n",
    "    folder_path = logs_path + folder\n",
    "    source_result_path = folder_path + '/results_LSTM.txt'\n",
    "#     print(f'Folder: {folder}, Results Text File: {source_result_path}')\n",
    "    merge_results(source_result_path, final_results_txt_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a3e00",
   "metadata": {},
   "source": [
    "The below code identifies the beginning and the end of a search combination in the log file and creates the content within that to be considered as a single search instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd804d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(final_results_txt_file_path, 'r') as results_file:\n",
    "    results_content = results_file.read()\n",
    "\n",
    "cleaned_content = [line for line in results_content.split('\\n')]\n",
    "\n",
    "search_combinations = []\n",
    "current_instance = []\n",
    "\n",
    "for line in cleaned_content:\n",
    "    if re.search(r'BEGIN SEARCH : for Script', line):\n",
    "        if current_instance:\n",
    "            search_combinations.append(current_instance)\n",
    "        current_instance = [line]\n",
    "    elif re.search(r'END SEARCH', line):\n",
    "        if current_instance:\n",
    "            current_instance.append(line)\n",
    "            search_combinations.append(current_instance)\n",
    "        current_instance = []\n",
    "    elif current_instance:\n",
    "        current_instance.append(line)\n",
    "\n",
    "# # Print the search_combinations\n",
    "# for idx, instance in enumerate(search_combinations, start=1):\n",
    "#     print(f\"Instance {idx}:\")\n",
    "#     for line in instance:\n",
    "#         print(line)\n",
    "#     print(\"\\n\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841135d",
   "metadata": {},
   "source": [
    "The below code iterates through all search combination instances and extracts the relevant information needed and stores it in the form of a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d579f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_info = []\n",
    "\n",
    "for search in search_combinations:\n",
    "    script_num = 0\n",
    "    search_count = 0\n",
    "    sequence_length = 0\n",
    "    unit = 0\n",
    "    dropout_rate = 0\n",
    "    activation_function = ''\n",
    "    loss_function = ''\n",
    "    optimizer = ''\n",
    "    num_epochs = 0\n",
    "    batch_size = 0\n",
    "    seed_value = 0\n",
    "    \n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    \n",
    "    model_parameters = ''\n",
    "    model_keys = ''\n",
    "    \n",
    "    classification_report = ''\n",
    "    capture_classification = False  # Flag to capture classification report content\n",
    "    confusion_matrix = ''\n",
    "\n",
    "    for line in search:\n",
    "        script_match = re.search(r'Script (\\d+)', line)\n",
    "        if script_match:\n",
    "            script_num = ast.literal_eval(script_match.group(1))\n",
    "        search_match = re.search(r'BEGIN SEARCH : (\\d+)', line)\n",
    "        if search_match:\n",
    "            search_count = ast.literal_eval(search_match.group(1))\n",
    "        sequence_length_match = re.search(r'Sequence Length = (\\d+)', line)\n",
    "        if sequence_length_match:\n",
    "            sequence_length = ast.literal_eval(sequence_length_match.group(1))\n",
    "        units_match = re.search(r'Units = (\\d+)', line)\n",
    "        if units_match:\n",
    "            units = ast.literal_eval(units_match.group(1))\n",
    "        dropout_match = re.search(r'Dropout = (\\d+\\.?\\d*)', line)\n",
    "        if dropout_match:\n",
    "            dropout_rate = ast.literal_eval((dropout_match.group(1)))\n",
    "        activation_match = re.search(r'Activation = (.+)', line)\n",
    "        if activation_match:\n",
    "            activation_function = activation_match.group(1)\n",
    "        loss_match = re.search(r'Loss Function = (.+)', line)\n",
    "        if loss_match:\n",
    "            loss_function = loss_match.group(1)\n",
    "        optimizer_match = re.search(r'Optimizer = (.+)', line)\n",
    "        if optimizer_match:\n",
    "            optimizer = optimizer_match.group(1)\n",
    "        epochs_match = re.search(r'Epochs = (\\d+)', line)\n",
    "        if epochs_match:\n",
    "            num_epochs = ast.literal_eval(epochs_match.group(1))\n",
    "        batch_size_match = re.search(r'Batch Size = (\\d+)', line)\n",
    "        if batch_size_match:\n",
    "            batch_size = ast.literal_eval(batch_size_match.group(1))\n",
    "        seed_match = re.search(r'Seed Value = (\\d+)', line)\n",
    "        if seed_match:\n",
    "            seed_value = ast.literal_eval(seed_match.group(1))\n",
    "\n",
    "        training_loss_match = re.search(r'Training Loss:\\s+(\\d+\\.\\d+)', line)\n",
    "        if training_loss_match:\n",
    "            training_loss = ast.literal_eval(training_loss_match.group(1))\n",
    "        training_accuracy_match = re.search(r'Training Accuracy:\\s+(\\d+\\.\\d+)', line)\n",
    "        if training_accuracy_match:\n",
    "            training_accuracy = ast.literal_eval(training_accuracy_match.group(1))\n",
    "        validation_loss_match = re.search(r'Validation Loss:\\s+(\\d+\\.\\d+)', line)\n",
    "        if validation_loss_match:\n",
    "            validation_loss = ast.literal_eval(validation_loss_match.group(1))\n",
    "        validation_accuracy_match = re.search(r'Validation Accuracy:\\s+(\\d+\\.\\d+)', line)\n",
    "        if validation_accuracy_match:\n",
    "            validation_accuracy = ast.literal_eval(validation_accuracy_match.group(1))\n",
    "        test_loss_match = re.search(r'Test Loss:\\s+(\\d+\\.\\d+)', line)\n",
    "        if test_loss_match:\n",
    "            test_loss = ast.literal_eval(test_loss_match.group(1))\n",
    "        test_accuracy_match = re.search(r'Test Accuracy:\\s+(\\d+\\.\\d+)', line)\n",
    "        if test_accuracy_match:\n",
    "            test_accuracy = ast.literal_eval(test_accuracy_match.group(1))\n",
    "\n",
    "        model_params_match = re.search(r\"Model Parameters: (.+)\", line)\n",
    "        if model_params_match:\n",
    "            model_parameters = ast.literal_eval(model_params_match.group(1))\n",
    "        model_keys_match = re.search(r\"Model Keys: (.+)\", line)\n",
    "        if model_keys_match:\n",
    "            model_keys = model_keys_match.group(1)\n",
    "\n",
    "        class_report_start = re.search(r'------------ CLASSIFICATION REPORT ------------', line)\n",
    "        if class_report_start:\n",
    "            capture_classification = True  # Start capturing content\n",
    "            continue  # Skip this line as it's just a marker\n",
    "        elif capture_classification and re.search(r'------------ CONFUSION MATRIX ------------', line):\n",
    "            capture_classification = False  # Stop capturing on \"Confusion Matrix\" marker\n",
    "        elif capture_classification:\n",
    "            if line.strip() != '':\n",
    "                classification_report += line + '\\n'\n",
    "\n",
    "        # Capture the lines within the \"CONFUSION MATRIX\" section\n",
    "        if re.search(r'Confusion matrix saved for', line):\n",
    "            confusion_matrix = line.split('Confusion matrix saved for ')[-1]  # Extract file name\n",
    "            break  # Stop capturing after saving the confusion matrix filename\n",
    "        \n",
    "        \n",
    "    capture_classification_row = False\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_scores = {}\n",
    "    macro_avgs = {}\n",
    "    weighted_avgs = {}\n",
    "\n",
    "    # RE pattern for obtaining classification reports values\n",
    "    class_values_pattern = r'^\\s*(\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+'\n",
    "    macro_avg_pattern = r'\\s*macro avg\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)'\n",
    "    weighted_avg_pattern = r'\\s*weighted avg\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)'\n",
    "\n",
    "    # Once we have the classification report, let us extract all the other information in it\n",
    "    for row in classification_report.split('\\n'):\n",
    "        row_headers_start_match = re.search(\"precision    recall  f1-score   support\", row)\n",
    "        if row_headers_start_match:\n",
    "            capture_classification_row = True\n",
    "            continue\n",
    "        elif capture_classification_row and re.search('macro avg', row):\n",
    "            macro_avg_match = re.search(macro_avg_pattern, row)\n",
    "            if macro_avg_match:\n",
    "                macro_avgs['precision'] = ast.literal_eval(macro_avg_match.group(1))\n",
    "                macro_avgs['recall'] = ast.literal_eval(macro_avg_match.group(2))\n",
    "                macro_avgs['f1_score'] = ast.literal_eval(macro_avg_match.group(3))\n",
    "                \n",
    "        elif capture_classification_row and re.search('weighted avg', row):\n",
    "            weighted_avg_match = re.search(weighted_avg_pattern, row)\n",
    "            if weighted_avg_match:\n",
    "                weighted_avgs['precision'] = ast.literal_eval(weighted_avg_match.group(1))\n",
    "                weighted_avgs['recall'] = ast.literal_eval(weighted_avg_match.group(2))\n",
    "                weighted_avgs['f1_score'] = ast.literal_eval(weighted_avg_match.group(3))\n",
    "                capture_classification_row = False\n",
    "#                 break\n",
    "        elif capture_classification_row and re.search(r'\\d+', row):\n",
    "            class_values_match = re.search(class_values_pattern, row)\n",
    "            if class_values_match:\n",
    "                class_label = ast.literal_eval(class_values_match.group(1))\n",
    "                precision_value = ast.literal_eval(class_values_match.group(2))\n",
    "                recall_value = ast.literal_eval(class_values_match.group(3))\n",
    "                f1_score = ast.literal_eval(class_values_match.group(4))\n",
    "                \n",
    "                precision_values[class_label] = precision_value\n",
    "                recall_values[class_label] = recall_value\n",
    "                f1_scores[class_label] = f1_score\n",
    "                \n",
    "        \n",
    "\n",
    "#     print(f'Script Number: {script_num}, Search Number: {search_count}')\n",
    "#     print(f'Sequence Length: {sequence_length}')\n",
    "#     print(f'Units: {units}')\n",
    "#     print(f'Dropout Rate: {dropout_rate}')\n",
    "#     print(f'Activation Function: {activation_function}')\n",
    "#     print(f'Loss Function: {loss_function}')\n",
    "#     print(f'Optimizer: {optimizer}')\n",
    "#     print(f'Number of Epochs: {num_epochs}')\n",
    "#     print(f'Batch Size: {batch_size}')\n",
    "#     print(f'Seed Value: {seed_value}')\n",
    "\n",
    "#     print(f'Training Loss: {training_loss}')\n",
    "#     print(f'Training Accuracy: {training_accuracy}')\n",
    "#     print(f'Validation Loss: {validation_loss}')\n",
    "#     print(f'Validation Accuracy: {validation_accuracy}')\n",
    "#     print(f'Test Loss: {test_loss}')\n",
    "#     print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "#     print(f'Model Parameters: {model_parameters}')\n",
    "#     print(f'Model Keys: {model_keys}')\n",
    "\n",
    "#     print(f'Classification Report:\\n{classification_report}')\n",
    "#     print(f'Confusion Matrix: {confusion_matrix}')\n",
    "#     print(f'Precison Values : {precision_values}')\n",
    "#     print(f'Recall Values: {recall_values}')\n",
    "#     print(f'F1-Scores: {f1_scores}')\n",
    "#     print(f'Macro Averages: {macro_avgs}')\n",
    "#     print(f'Weighted Averages: {weighted_avgs}')\n",
    "\n",
    "    # Append all the collected information to the search_info list if needed\n",
    "    search_info.append({\n",
    "        'script_num': script_num,\n",
    "        'search_count': search_count,\n",
    "        'sequence_length': sequence_length,\n",
    "        'units': units,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'activation_function': activation_function,\n",
    "        'loss_function': loss_function,\n",
    "        'optimizer': optimizer,\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'seed_value': seed_value,\n",
    "        'training_loss': training_loss,\n",
    "        'training_accuracy': training_accuracy,\n",
    "        'validation_loss': validation_loss,\n",
    "        'validation_accuracy': validation_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision_values': precision_values,\n",
    "        'recall_values': recall_values,\n",
    "        'f1_scores': f1_scores,\n",
    "        'macro_averages': macro_avgs,\n",
    "        'weighted_averages': weighted_avgs,\n",
    "        'model_parameters': model_parameters,\n",
    "        'model_keys': model_keys,\n",
    "        'classification_report': classification_report,\n",
    "        'confusion_matrix': confusion_matrix\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971a998",
   "metadata": {},
   "source": [
    "The below code converts the search_combinations_information dictionary to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150bee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\n",
    "    'script_num',\n",
    "    'search_count',\n",
    "    'sequence_length',\n",
    "    'units',\n",
    "    'dropout_rate',\n",
    "    'activation_function',\n",
    "    'loss_function',\n",
    "    'optimizer',\n",
    "    'num_epochs',\n",
    "    'batch_size',\n",
    "    'seed_value',\n",
    "    'training_loss',\n",
    "    'training_accuracy',\n",
    "    'validation_loss',\n",
    "    'validation_accuracy',\n",
    "    'test_loss',\n",
    "    'test_accuracy',\n",
    "    'precision_values',\n",
    "    'recall_values',\n",
    "    'f1_scores',\n",
    "    'macro_averages',\n",
    "    'weighted_averages',\n",
    "    'model_parameters',\n",
    "    'model_keys',\n",
    "    'classification_report',\n",
    "    'confusion_matrix'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(search_info, columns = df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b37ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df_path = final_results_file_path +  '/lstm_script_2_gridSearch_results.csv'\n",
    "df_no_duplicates = df.drop_duplicates(subset=['search_count'])\n",
    "df_no_duplicates.to_csv(f'{final_results_df_path}', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1534733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
