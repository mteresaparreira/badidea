{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e42600d",
   "metadata": {},
   "source": [
    "\n",
    "Description: This script trains a LSTM Model on the facial data extracted from the OpenFace library to classify human reactions into Control, Failure Human, Failure Robot classes  \n",
    "\n",
    "- Author: Sukruth Gowdru Lingaraju\n",
    "- Date Created: August 18th, 2023\n",
    "- Python Version: 3.10.9\n",
    "- Email: sg2257@cornell.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd487f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8b00eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is using GPU support\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493244d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnd: Method Definitions\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Begin: Method Definitions\n",
    "\"\"\"\n",
    "# def classification_report_tolerance(y_pred, y_true, margin = 1):\n",
    "    \n",
    "#     metrics_dict = dict()\n",
    "#     all_metrics_dict = dict()\n",
    "#     classes = np.unique(y_true)\n",
    "#     all_p = []\n",
    "#     all_r = []\n",
    "#     all_a = []\n",
    "#     all_f1 = []\n",
    "    \n",
    "#     for classi in classes:\n",
    "#         tp = 0\n",
    "#         tn = 0\n",
    "#         fp = 0\n",
    "#         fn = 0\n",
    "\n",
    "\n",
    "#         for i, y in enumerate(y_pred):\n",
    "#             if y == classi:\n",
    "#                 if y in y_true[i-margin:i+margin+1]:\n",
    "#                     #print(y_true[i-1:i+1].shape)\n",
    "#                     tp = tp + 1\n",
    "#                 else:\n",
    "#                     fp = fp + 1\n",
    "#             else:\n",
    "#                 if y not in y_true[i-1:i+1]:\n",
    "#                     fn = fn + 1\n",
    "#                 else:\n",
    "#                     tn = tn + 1\n",
    "#         precision = tp/(tp+fp)\n",
    "#         recall = tp / (tp + fn)\n",
    "#         f1 = (2*precision*recall)/(precision+recall)\n",
    "#         accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "#         metrics_dict[classi] = [tp, tn, fp, fp]\n",
    "#         all_metrics_dict[classi] = [precision, recall, f1, accuracy]\n",
    "#         all_p.append(precision)\n",
    "#         all_r.append(recall)\n",
    "#         all_a.append(accuracy)\n",
    "#         all_f1.append(f1)\n",
    "        \n",
    "    \n",
    "#     print(metrics_dict)\n",
    "#     print(all_metrics_dict)\n",
    "    \n",
    "#     macro_dict = dict()\n",
    "#     macro_dict['macro-precision'] = sum(np.array(all_p))/len(all_p)\n",
    "#     macro_dict['macro-recall'] = sum(np.array(all_r))/len(all_r)\n",
    "#     macro_dict['macro-accuracy'] = sum(np.array(all_a))/len(all_a)\n",
    "#     macro_dict['macro-f1'] = (2* macro_dict['macro-precision']*macro_dict['macro-recall'])/(macro_dict['macro-recall'] + macro_dict['macro-precision'])\n",
    "    \n",
    "#     print(macro_dict)\n",
    "    \n",
    "#     return metrics_dict, all_metrics_dict, macro_dict\n",
    "\n",
    "def createDataSplits(df, results_directory= '.', seed_value = 42, sequence_length = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    createDataSplits(): accepts a dataframe along with the directory to store the results and sequence_length - to perform data splits for training, validation, and testing\n",
    "\n",
    "    Parameters:\n",
    "    - df\n",
    "    - results_directory: to write the 'Exception Error' thrown if there are any problems in splitting the data\n",
    "    - seed_value\n",
    "    - sequence_length (a.k.a: lookbacks)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # # Set seed\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        tf.random.set_seed(seed_value)\n",
    "\n",
    "        # # Extract features and labels\n",
    "\n",
    "        # # for naive & naive_n datasets\n",
    "        # features = df.iloc[:, 3:]\n",
    "        # target_class = df['class'].values\n",
    "\n",
    "        # # for full & full_n datasets\n",
    "        features = df.iloc[:, 4:]\n",
    "        target_class = df.iloc[:, 2].values\n",
    "        target_class = target_class.astype('int')\n",
    "        \n",
    "        \"\"\"\n",
    "        Begin: K-Fold Cross-Validation splits\n",
    "        \n",
    "        Identify the range of the splits & assign participants belonging to those ranges to their respective folds\n",
    "        - 'start_test_indx', 'end_test_indx': defines the range of the particiapants belonging to the 'test_fold'\n",
    "        - 'test_fold': consists of the participants belonging to the 'k'th fold\n",
    "        - 'remaining_participants': set difference between original 'participants' & 'test_fold' participants\n",
    "        - 'val_fold': consists of 'val_fold_size' participants randomly shuffled after obtaining 'remaining_participants' belonging to the 'k'th fold\n",
    "        - 'train_fold': consists of all the remaining participants belonging to the 'k'th fold\n",
    "        - 'test_folds', 'val_folds', 'train_folds': consists of the set of participants in each fold\n",
    "        \"\"\"\n",
    "        \n",
    "        participants = np.unique(df['participant_id'])\n",
    "\n",
    "        #Number of participants for train, validation, and test\n",
    "        train_fold_size = 20\n",
    "        val_fold_size = 3\n",
    "        test_fold_size = 6\n",
    "\n",
    "        #number of dataset folds\n",
    "        num_folds = 5\n",
    "\n",
    "        # Shuffle the list of participants\n",
    "        np.random.shuffle(participants)\n",
    "\n",
    "        # Initialize lists to store train, validation, and test participants for each fold\n",
    "        train_folds = []\n",
    "        val_folds = []\n",
    "        test_folds = []\n",
    "\n",
    "        # Create non-overlapping test folds and validation folds\n",
    "        for i in range(num_folds):\n",
    "            start_test_idx = i * test_fold_size\n",
    "            end_test_idx = start_test_idx + np.min([test_fold_size, len(participants) - start_test_idx])\n",
    "\n",
    "            test_fold = participants[start_test_idx:end_test_idx]\n",
    "               \n",
    "            # Identify all the participants except the participants belonging to the test_fold & shuffle them\n",
    "            remaining_participants = np.setdiff1d(participants, test_fold)\n",
    "            np.random.shuffle(remaining_participants)\n",
    "            \n",
    "            # Validation set selected from the remaining participants\n",
    "            val_fold = remaining_participants[:val_fold_size]\n",
    "            \n",
    "            # Identify all the participants that don't belong to 'val_fold' & 'test_fold' and assign them to the 'train_fold'\n",
    "            train_fold = np.setdiff1d(remaining_participants, val_fold)\n",
    "            \n",
    "            # Append the participant sets to their corresponding folds\n",
    "            train_folds.append(train_fold)\n",
    "            val_folds.append(val_fold)\n",
    "            test_folds.append(test_fold)\n",
    "        \n",
    "        \"\"\"\n",
    "        End: K-Fold Cross-Validation splits\n",
    "        \"\"\"\n",
    "\n",
    "        # # Create train, validation, and test sets for each fold in 'num_folds'\n",
    "        # For now, do only for fold: '0'\n",
    "        train_fold = train_folds[0]\n",
    "        val_fold = val_folds[0]\n",
    "        test_fold = test_folds[0]\n",
    "        \n",
    "        \"\"\"\n",
    "        Begin: train, val, test: splits & sequences\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the data into train, validation, and test sets\n",
    "        train_set = df[df['participant_id'].isin(train_fold)]\n",
    "        X_train = features.loc[train_set.index, : ]\n",
    "        \n",
    "        val_set = df[df['participant_id'].isin(val_fold)]\n",
    "        X_val = features.loc[val_set.index, : ]\n",
    "        \n",
    "        test_set = df[df['participant_id'].isin(test_fold)]\n",
    "        X_test  = features.loc[test_set.index, : ]\n",
    "\n",
    "        # Convert labels to categorical format\n",
    "        num_classes = np.max(target_class) + 1  # Assuming labels start from 0\n",
    "        labels_ohe = np.eye(num_classes)[target_class]\n",
    "        \n",
    "        # Retrieve y_train, y_val, and y_test: values corresponding to same indexes, from labels_ohe\n",
    "        y_train = labels_ohe[X_train.index]\n",
    "        y_val = labels_ohe[X_val.index]\n",
    "        y_test = labels_ohe[X_test.index]\n",
    "\n",
    "#         # Print size of all sets\n",
    "#         print('Size of all sets before resetting the X indexes')\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "#         print(X_val.shape, y_val.shape)\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "\n",
    "        #reset indexes\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_val = X_val.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        \n",
    "#         # Print size of all sets after resetting the X indexes\n",
    "#         print('Size of all sets after resetting the X indexes')\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "#         print(X_val.shape, y_val.shape)\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "        \n",
    "        #### Split data into train and test sets: if k-fold cross-validation is not needed\n",
    "        ### X_train, X_test, y_train, y_test = train_test_split(features, labels_ohe, test_size=0.2, random_state=seed_value)\n",
    "        \n",
    "        \"\"\"\n",
    "        Begin: Sequence Creation as per defined 'sequence_length'(a.k.a: lookbacks)\n",
    "        \"\"\"\n",
    "        \n",
    "        X_train_sequences = [X_train[i : i + sequence_length] for i in range(len(X_train) - sequence_length + 1)]\n",
    "        y_train_sequences = y_train[sequence_length - 1 : ]\n",
    "\n",
    "        X_val_sequences = [X_val[i : i + sequence_length] for i in range(len(X_val) - sequence_length + 1)]\n",
    "        y_val_sequences = y_val[sequence_length - 1 : ]\n",
    "\n",
    "        X_test_sequences = [X_test[i : i + sequence_length] for i in range(len(X_test) - sequence_length + 1)]\n",
    "        y_test_sequences = y_test[sequence_length - 1 : ]\n",
    "        \n",
    "        \"\"\"\n",
    "        End: Sequence Creation\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        End: train, val, test: splits & sequences\n",
    "        \"\"\"\n",
    "        return num_classes, X_train, X_val, X_test, y_train, y_val, y_test, X_train_sequences, y_train_sequences, X_val_sequences, y_val_sequences, X_test_sequences, y_test_sequences\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "            results_file.write(\n",
    "                f'Exception {e} thrown during splitting the dataset for :- '\n",
    "                f'{traceback.print_exc()}'\n",
    "            )\n",
    "        pass\n",
    "\n",
    "def plot_batchSize_accuracy(train_accuracy, val_accuracy, batch_size, axs):\n",
    "    \"\"\"\n",
    "    plot_batchSize_accuracy(): creates subplots of training & validation accuracy scores for varying batch sizes\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_accuracy) + 1)\n",
    "    axs.plot(epochs, train_accuracy, label='Training Accuracy')\n",
    "    axs.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "    axs.set_title(f'Batch Size: {batch_size}')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_ylabel('Accuracy')\n",
    "    axs.legend()\n",
    "\n",
    "def executeModel_LSTM(df, results_directory, seed_value = 42, sequence_lengths = 1, units = 64, dropouts = 0.2, activations = 'softmax', losses = 'categorical_crossentropy', optimizers = 'adam', epochs = 100, batch_sizes = 32):\n",
    "\n",
    "    \"\"\"\n",
    "    executeModel_LSTM(): takes in the dataFrame along with the directory specification & hyper-parameters and trains a LSTM model\n",
    "\n",
    "    Parameters:\n",
    "    - df\n",
    "    - results_directory\n",
    "    - seed_value\n",
    "    - sequence_length (a.k.a: lookback)\n",
    "    - units\n",
    "    - dropouts\n",
    "    - activations\n",
    "    - losses\n",
    "    - optimizers\n",
    "    - epochs\n",
    "    \"\"\"\n",
    "    # Keep count of the number of different search combinations\n",
    "    search_count = 0\n",
    "\n",
    "    # Create a figure with subplots for plotting the training and validation accuracy for various batch_sizes against the # of epochs\n",
    "    num_rows = 2\n",
    "    num_cols = 2\n",
    "    batch_figure, axs = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "    \n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    for sequence_length in sequence_lengths:\n",
    "        for unit in units:\n",
    "            for dropout in dropouts:\n",
    "                for activation in activations:\n",
    "                    for loss in losses:\n",
    "                        for optimizer in optimizers:\n",
    "                            for epoch in epochs:\n",
    "                                for i, batch_size in enumerate(batch_sizes):\n",
    "                                    try:\n",
    "                                        search_count += 1\n",
    "                                        \n",
    "                                        # # Retrieve the splits\n",
    "                                        num_classes, X_train, X_val, X_test, y_train, y_val, y_test, X_train_sequences, y_train_sequences, X_val_sequences, y_val_sequences, X_test_sequences, y_test_sequences = createDataSplits(df, results_directory, seed_value, sequence_length)\n",
    "                                        \n",
    "                                        \"\"\"\n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                        Begin: Model Architecture\n",
    "                                        \"\"\"\n",
    "                                        # # Check GPU availability\n",
    "                                        # gpus = tf.config.list_physical_devices('GPU')\n",
    "                                        # print(\"GPU available:\", gpus)\n",
    "                                        # # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "                                        # # Assuming there is at least one GPU available\n",
    "                                        # if gpus:\n",
    "                                        #     # Set TensorFlow to use GPU 0\n",
    "                                        #     try:\n",
    "                                        #         tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "                                        #         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                                        #         # gpu_device = tf.config.list_physical_devices('GPU')[0]\n",
    "                                        #         # tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "                                        #         # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "                                        #     except RuntimeError as e:\n",
    "                                        #         print(e)\n",
    "                                                \n",
    "                                        # # Specify the device used for computation\n",
    "                                        with tf.device('/GPU:0'):  # Use GPU 0\n",
    "\n",
    "                                            # Build and train your model here\n",
    "\n",
    "                                            # # Create the LSTM model\n",
    "                                            model = Sequential()\n",
    "                                            model.add(LSTM(units = unit, input_shape = (sequence_length, X_train.shape[1])))\n",
    "                                            model.add(Dropout(dropout))\n",
    "                                            model.add(Dense(units=num_classes, activation = activation))\n",
    "\n",
    "                                            # Compile the model\n",
    "                                            model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "                                            # Train the model and capture the history data\n",
    "                                            model_history = model.fit(\n",
    "                                                np.array(X_train_sequences),\n",
    "                                                y_train_sequences,\n",
    "                                                batch_size = batch_size,\n",
    "                                                epochs = epoch,\n",
    "                                                verbose = 'auto',\n",
    "                                                validation_data=(np.array(X_val_sequences), y_val_sequences),\n",
    "                                            )\n",
    "                                            \n",
    "                                            # Obtain the training loss & accuracy data\n",
    "                                            train_loss, train_accuracy = model_history.history['loss'], model_history.history['accuracy']\n",
    "                                            \n",
    "                                            # Obtain the validation loss & accuracy data\n",
    "                                            val_loss, val_accuracy = model_history.history['val_loss'], model_history.history['val_accuracy']\n",
    "                                            \n",
    "                                            # Evaluate the model on test data\n",
    "                                            test_loss, test_accuracy = model.evaluate(np.array(X_test_sequences), y_test_sequences)\n",
    "\n",
    "                                            \"\"\"\n",
    "                                            Save the model information data as an object\n",
    "                                            Define the path to store the object data & create the directory if it does not exist\n",
    "                                            \"\"\"\n",
    "                                            model_data_path = results_directory + 'model_data/'\n",
    "                                            \n",
    "                                            if not os.path.exists(model_data_path):\n",
    "                                                os.makedirs(model_data_path)\n",
    "\n",
    "                                            # Store the model learning data using pickle\n",
    "                                            model_data_information = {\n",
    "                                                'train_loss': train_loss,\n",
    "                                                'train_accuracy': train_accuracy,\n",
    "                                                'val_loss': val_loss,\n",
    "                                                'val_accuracy': val_accuracy,\n",
    "                                                'test_loss': test_loss,\n",
    "                                                'test_accuracy': test_accuracy\n",
    "                                            }\n",
    "\n",
    "                                            with open(model_data_path + f'model_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}_{batch_size}', 'wb') as f:\n",
    "                                                pickle.dump(model_data_information, f)\n",
    "                                            \n",
    "                                            \"\"\"\n",
    "                                            End: Model Architecture\n",
    "                                            ------------------------------------------------------------------------------------\n",
    "                                            \"\"\"\n",
    "                                            \"\"\"\n",
    "                                            ------------------------------------------------------------------------------------\n",
    "                                                Predictions using the Model\n",
    "                                                ===========================\n",
    "\n",
    "                                                When making predictions using the trained model, the output is in the form of predicted probabilities,\n",
    "                                                indicating the likelihood of each sample belonging to each target class.\n",
    "\n",
    "                                                Predicted Probabilities (y_predict_probs):\n",
    "                                                - Shape: (#samples, #target_classes)\n",
    "                                                - Each value in y_predict_probs represents the probability of the corresponding sample being classified\n",
    "                                                into the respective class.\n",
    "\n",
    "                                                Converting Probabilities to Class Labels (y_predict):\n",
    "                                                - The y_predict array is derived by finding the index of the maximum value along a specified axis.\n",
    "                                                - It represents the predicted class label for each sample based on the highest predicted probability.\n",
    "                                            ------------------------------------------------------------------------------------\n",
    "                                            \"\"\"\n",
    "\n",
    "                                            y_predict_probs = model.predict(np.array(X_test_sequences))\n",
    "                                            y_predict = np.argmax(y_predict_probs, axis=1)  # Convert to class labels\n",
    "\n",
    "                                            report = classification_report(np.argmax(y_test_sequences, axis=1), y_predict)\n",
    "\n",
    "                                            \"\"\"\n",
    "                                                Generate the Confusion Matrix\n",
    "                                            \"\"\"\n",
    "\n",
    "                                            # Calculate the confusion matrix\n",
    "                                            conf_matrix = confusion_matrix(np.argmax(y_test_sequences, axis=1), y_predict)\n",
    "\n",
    "                                            # Get the class labels (assuming y_true and y_pred are integer class labels)\n",
    "                                            class_labels = ['Control', 'Failure_Human', 'Failure_Robot']\n",
    "\n",
    "                                            # Plot the confusion matrix using seaborn and matplotlib\n",
    "                                            plt.figure(figsize=(8, 8))\n",
    "                                            sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "                                            plt.xlabel(\"Predicted\")\n",
    "                                            plt.ylabel(\"True\")\n",
    "                                            plt.title(\"Confusion Matrix\")\n",
    "\n",
    "                                            # Save the confusion matrix as an image\n",
    "                                            confusion_matrix_path = results_directory + f'confusion_matrices/'\n",
    "                                            \n",
    "                                            if not os.path.exists(confusion_matrix_path):\n",
    "                                                os.makedirs(confusion_matrix_path)\n",
    "                                            \n",
    "                                            confusion_matrix_path = results_directory + f'confusion_matrices/confusion_matrix_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}_{batch_size}.png'\n",
    "                                            plt.savefig(confusion_matrix_path)\n",
    "                                            plt.clf()\n",
    "                                            \n",
    "                                            # # Plot training and validation accuracy plots for varying batch_sizes\n",
    "                                            # Plot training and validation accuracy on the current subplot\n",
    "\n",
    "                                            # Calculate row and column indices for the current subplot\n",
    "                                            row_idx = i // num_cols\n",
    "                                            col_idx = i % num_cols\n",
    "                                            \n",
    "                                            # Get the current subplot\n",
    "                                            ax = axs[row_idx, col_idx]\n",
    "                                            ax.clear()\n",
    "                                            plot_batchSize_accuracy(train_accuracy, val_accuracy, batch_size, ax)\n",
    "\n",
    "                                            \"\"\"\n",
    "                                            ------------------------------------------------------------------------------------\n",
    "                                            Begin: Logging \n",
    "                                            - Write all the information of the particular combination of the model to a file below\n",
    "                                            \"\"\"\n",
    "\n",
    "                                            with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "                                                results_file.write(\"\\n\")\n",
    "                                                results_file.write(f\"------------ BEGIN SEARCH : {search_count} ------------\" + \"\\n\")\n",
    "                                                results_file.write(\"------------ TYPE ------------\" + \"\\n\")\n",
    "\n",
    "                                                results_file.write(\n",
    "                                                    f'Sequence Length = {sequence_length}\\n'\n",
    "                                                    f'Units = {unit}\\n'\n",
    "                                                    f'Dropout = {dropout}\\n'\n",
    "                                                    f'Activation = {activation}\\n'\n",
    "                                                    f'Loss Function = {loss}\\n'\n",
    "                                                    f'Optimizer = {optimizer}\\n'\n",
    "                                                    f'Epochs = {epoch}\\n'\n",
    "                                                    f'Batch Size = {batch_size}\\n'\n",
    "                                                    f'Seed Value = {seed_value}\\n'\n",
    "                                                )\n",
    "\n",
    "                                                results_file.write(\"------------ METRICS ------------\" + \"\\n\")\n",
    "\n",
    "                                                results_file.write(f'Training Loss: {train_loss[-1]: .4f}' + '\\n')\n",
    "                                                results_file.write(f'Training Accuracy: {train_accuracy[-1]: .4f}' + '\\n')\n",
    "                                                results_file.write(f'Validation Loss: {val_loss[-1]: .4f}' + '\\n')\n",
    "                                                results_file.write(f'Validation Accuracy: {val_accuracy[-1]: .4f}' + '\\n')\n",
    "                                                results_file.write(f'Test Loss: {test_loss:.4f}' + '\\n')\n",
    "                                                results_file.write(f'Test Accuracy: {test_accuracy:.4f}' + '\\n')\n",
    "\n",
    "                                                results_file.write(\"------------ PARAMETERS ------------\" + \"\\n\")\n",
    "\n",
    "                                                results_file.write(f'Model Parameters: {model_history.params}' + '\\n')\n",
    "                                                results_file.write(f'Model Keys: {model_history.history.keys()}' + '\\n')\n",
    "\n",
    "                                                results_file.write(\"------------ CLASSIFICATION REPORT ------------\" + \"\\n\")\n",
    "\n",
    "                                                results_file.write(report + '\\n')\n",
    "\n",
    "                                                results_file.write(\"------------ CONFUSION MATRIX ------------\" + \"\\n\")\n",
    "\n",
    "                                                results_file.write(f'Confusion matrix saved for confusion_matrix_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}_{batch_size}.png' + '\\n')\n",
    "                                                results_file.write(f\"------------ END SEARCH : {search_count} ------------\" + \"\\n\")\n",
    "                                                results_file.write(\"\\n\")\n",
    "\n",
    "                                    except Exception as e:\n",
    "                                        with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "                                            results_file.write(\n",
    "                                                f'Exception {e} thrown for :- \\n'\n",
    "                                                f'{traceback.print_exc()} \\n'\n",
    "                                                f'Sequence Length = {sequence_length}\\n'\n",
    "                                                f'Units = {unit}\\n'\n",
    "                                                f'Dropout = {dropout}\\n'\n",
    "                                                f'Activation = {activation}\\n'\n",
    "                                                f'Loss Function = {loss}\\n'\n",
    "                                                f'Optimizer = {optimizer}\\n'\n",
    "                                                f'Epochs = {epoch}\\n'\n",
    "                                                f'Batch Size = {batch_size}\\n'\n",
    "                                                f'Seed Value = {seed_value}\\n'\n",
    "                                            )\n",
    "                                    # break # batch_size break\n",
    "                                \"\"\"\n",
    "                                BEGIN: Subplots for training and validation accuracy for varying batch_sizes\n",
    "                                \"\"\"\n",
    "                                # Adjust layout and save the figure for training & validation accuracy for varying batch_sizes\n",
    "                                batch_size_results_path = results_directory + 'batch_size_results/'\n",
    "                    \n",
    "                                if not os.path.exists(batch_size_results_path):\n",
    "                                    os.makedirs(batch_size_results_path)\n",
    "\n",
    "                                batch_size_accuracy_path = batch_size_results_path + f'batch_size_comparison_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}.png'\n",
    "                                batch_figure.tight_layout(pad=2.5)\n",
    "                                batch_figure.savefig(batch_size_accuracy_path)\n",
    "                                # plt.show()\n",
    "                                plt.close(batch_figure)\n",
    "                                plt.clf()\n",
    "\n",
    "                                with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "                                    results_file.write(\"------------ BATCH_SIZE PLOTS ------------\" + \"\\n\")\n",
    "                                    results_file.write(f'BATCH_SIZE plots saved for batch_size_comparison_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}.png' + '\\n')\n",
    "                                    results_file.write(\"------------ BATCH_SIZE PLOTS ------------\" + \"\\n\")\n",
    "                                \"\"\"\n",
    "                                End: Logging \n",
    "                                ------------------------------------------------------------------------------------\n",
    "                                End: batch_size subplots\n",
    "                                \"\"\"\n",
    "                                # break # epochs break\n",
    "                            break # optimizers break\n",
    "                        break # losses break\n",
    "                    break # activation functions break\n",
    "                break # dropouts break\n",
    "            break # units break\n",
    "        break # sequence_lengths break\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Begin: Directories specification\n",
    "    \"\"\"\n",
    "    \n",
    "    # allParticipants dataset path\n",
    "    superBAD_df = pd.read_csv('../data/allParticipant_data/allParticipants_5fps_downsampled_preprocessed_norm.csv')\n",
    "    \n",
    "    # results directory - make a new folder with the day and time of the run\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    results_directory = '../results/' + 'LSTM_' + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + '/'\n",
    "    \n",
    "    # Create 'results_directory' if it doesn't exist\n",
    "    if not os.path.exists(results_directory):\n",
    "        os.makedirs(results_directory)\n",
    "    \n",
    "    \"\"\"\n",
    "    End: Directories specification\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Begin: Hyperparameters definition\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [5, 10, 15]\n",
    "    units = [32, 64, 128]\n",
    "    dropouts = [0, 0.2, 0.4, 0.6]\n",
    "    activations = ['sigmoid', 'relu', 'tanh', 'softmax']\n",
    "    losses = ['categorical_crossentropy', 'sparse_categorical_crossentropy', 'binary_crossentropy', 'hinge']\n",
    "    optimizers = ['SGD', 'Adam']\n",
    "    epochs = [1, 2, 3, 4]\n",
    "    batch_sizes = [2048, 4096, 8192, 16384]\n",
    "\n",
    "    \"\"\"\n",
    "    End: Hyperparameters definition\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Begin: Call methods\n",
    "    \"\"\"\n",
    "\n",
    "    # Call your model execution function with keyword arguments\n",
    "    executeModel_LSTM(\n",
    "        superBAD_df,\n",
    "        results_directory,\n",
    "        seed_value = 42,\n",
    "        sequence_lengths = sequence_lengths,\n",
    "        units = units,\n",
    "        dropouts = dropouts,\n",
    "        activations = activations,\n",
    "        losses = losses,\n",
    "        optimizers = optimizers,\n",
    "        epochs = epochs,\n",
    "        batch_sizes = batch_sizes\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    End: Call methods\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    "End: Method Definitions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37557304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 33ms/step - loss: 1.1258 - accuracy: 0.3238 - val_loss: 1.1310 - val_accuracy: 0.3609\n",
      "280/280 [==============================] - 1s 4ms/step - loss: 1.1516 - accuracy: 0.3372\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 59ms/step - loss: 1.1358 - accuracy: 0.3053 - val_loss: 1.1432 - val_accuracy: 0.3415\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1599 - accuracy: 0.3347\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 1.1411 - accuracy: 0.2963 - val_loss: 1.1507 - val_accuracy: 0.3244\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1648 - accuracy: 0.3325\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 1.1438 - accuracy: 0.2919 - val_loss: 1.1547 - val_accuracy: 0.3145\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1674 - accuracy: 0.3320\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/2\n",
      "15/15 [==============================] - 1s 29ms/step - loss: 1.1258 - accuracy: 0.3238 - val_loss: 1.1310 - val_accuracy: 0.3609\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1.0889 - accuracy: 0.4031 - val_loss: 1.1091 - val_accuracy: 0.3901\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1361 - accuracy: 0.3470\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - 1s 60ms/step - loss: 1.1358 - accuracy: 0.3053 - val_loss: 1.1432 - val_accuracy: 0.3415\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1140 - accuracy: 0.3455 - val_loss: 1.1295 - val_accuracy: 0.3649\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1506 - accuracy: 0.3376\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - 1s 110ms/step - loss: 1.1411 - accuracy: 0.2963 - val_loss: 1.1507 - val_accuracy: 0.3244\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 1.1295 - accuracy: 0.3158 - val_loss: 1.1431 - val_accuracy: 0.3415\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1598 - accuracy: 0.3348\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 1s 313ms/step - loss: 1.1438 - accuracy: 0.2919 - val_loss: 1.1547 - val_accuracy: 0.3145\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.1378 - accuracy: 0.3008 - val_loss: 1.1507 - val_accuracy: 0.3239\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1648 - accuracy: 0.3326\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/3\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 1.1258 - accuracy: 0.3238 - val_loss: 1.1310 - val_accuracy: 0.3609\n",
      "Epoch 2/3\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1.0889 - accuracy: 0.4031 - val_loss: 1.1091 - val_accuracy: 0.3901\n",
      "Epoch 3/3\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1.0594 - accuracy: 0.4659 - val_loss: 1.0918 - val_accuracy: 0.4280\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1231 - accuracy: 0.3588\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 1s 58ms/step - loss: 1.1358 - accuracy: 0.3053 - val_loss: 1.1432 - val_accuracy: 0.3415\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1140 - accuracy: 0.3455 - val_loss: 1.1295 - val_accuracy: 0.3649\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.0947 - accuracy: 0.3906 - val_loss: 1.1174 - val_accuracy: 0.3805\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1422 - accuracy: 0.3431\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 1s 117ms/step - loss: 1.1411 - accuracy: 0.2963 - val_loss: 1.1507 - val_accuracy: 0.3244\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1.1295 - accuracy: 0.3158 - val_loss: 1.1431 - val_accuracy: 0.3415\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.1187 - accuracy: 0.3347 - val_loss: 1.1360 - val_accuracy: 0.3514\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1550 - accuracy: 0.3357\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/3\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 1.1438 - accuracy: 0.2919 - val_loss: 1.1547 - val_accuracy: 0.3145\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1378 - accuracy: 0.3008 - val_loss: 1.1507 - val_accuracy: 0.3239\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.1321 - accuracy: 0.3118 - val_loss: 1.1469 - val_accuracy: 0.3345\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1623 - accuracy: 0.3336\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/4\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 1.1258 - accuracy: 0.3238 - val_loss: 1.1310 - val_accuracy: 0.3609\n",
      "Epoch 2/4\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1.0889 - accuracy: 0.4031 - val_loss: 1.1091 - val_accuracy: 0.3901\n",
      "Epoch 3/4\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1.0594 - accuracy: 0.4659 - val_loss: 1.0918 - val_accuracy: 0.4280\n",
      "Epoch 4/4\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 1.0357 - accuracy: 0.5123 - val_loss: 1.0781 - val_accuracy: 0.4671\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1121 - accuracy: 0.3809\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/4\n",
      "8/8 [==============================] - 1s 57ms/step - loss: 1.1358 - accuracy: 0.3053 - val_loss: 1.1432 - val_accuracy: 0.3415\n",
      "Epoch 2/4\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.1140 - accuracy: 0.3455 - val_loss: 1.1295 - val_accuracy: 0.3649\n",
      "Epoch 3/4\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0947 - accuracy: 0.3906 - val_loss: 1.1174 - val_accuracy: 0.3805\n",
      "Epoch 4/4\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0776 - accuracy: 0.4286 - val_loss: 1.1068 - val_accuracy: 0.3928\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1345 - accuracy: 0.3477\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/4\n",
      "4/4 [==============================] - 1s 121ms/step - loss: 1.1411 - accuracy: 0.2963 - val_loss: 1.1507 - val_accuracy: 0.3244\n",
      "Epoch 2/4\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1.1295 - accuracy: 0.3158 - val_loss: 1.1431 - val_accuracy: 0.3415\n",
      "Epoch 3/4\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.1187 - accuracy: 0.3347 - val_loss: 1.1360 - val_accuracy: 0.3514\n",
      "Epoch 4/4\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.1084 - accuracy: 0.3567 - val_loss: 1.1293 - val_accuracy: 0.3649\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1505 - accuracy: 0.3377\n",
      "280/280 [==============================] - 1s 3ms/step\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 1s 344ms/step - loss: 1.1438 - accuracy: 0.2919 - val_loss: 1.1547 - val_accuracy: 0.3145\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.1378 - accuracy: 0.3008 - val_loss: 1.1507 - val_accuracy: 0.3239\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1321 - accuracy: 0.3118 - val_loss: 1.1469 - val_accuracy: 0.3345\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.1265 - accuracy: 0.3202 - val_loss: 1.1431 - val_accuracy: 0.3415\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 1.1598 - accuracy: 0.3348\n",
      "280/280 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11ac62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 26 2023, 14:05:08) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
